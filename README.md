# Web Scraper SUAP - Coleta de Links de Processos

Script automatizado para buscar processos no sistema SUAP do IFSP e extrair links diretos, salvando os resultados em arquivo CSV.

## Caracter√≠sticas

‚úÖ **Robusto**: Usa seletores est√°veis (ID e CSS) em vez de XPath absoluto  
‚úÖ **Tratamento de Erros**: Continua funcionando mesmo se alguns processos n√£o forem encontrados  
‚úÖ **Espera Inteligente**: Aguarda elementos carregarem antes de interagir  
‚úÖ **Logging Detalhado**: Acompanhe o progresso e identifique problemas  
‚úÖ **Entrada JSON**: L√™ automaticamente do arquivo `lista.json`  
‚úÖ **Flex√≠vel**: Aceita listas, arquivos TXT ou CSV como entrada

## Instala√ß√£o

### 1. Instalar Python (se n√£o tiver)

```bash
# Baixe do site oficial: https://python.org
```

### 2. Instalar ChromeDriver

```bash
# Op√ß√£o 1: Download manual
# Baixe de: https://chromedriver.chromium.org
# Adicione ao PATH do sistema

# Op√ß√£o 2: Usando webdriver-manager (recomendado)
pip install webdriver-manager
```

### 3. Instalar depend√™ncias

```bash
pip install -r requirements.txt
```

## Uso B√°sico

### M√©todo 1: Usar o arquivo lista.json (Recomendado)

```bash
# O script j√° est√° configurado para ler do lista.json
python web_scraper_suap.py

# O navegador abrir√° automaticamente
# Fa√ßa seu login no SUAP
# Pressione ENTER no terminal para continuar
```

### M√©todo 2: Programaticamente

```python
from web_scraper_suap import SUAPScraper, carregar_processos_json

# Carregar do arquivo JSON
processos = carregar_processos_json('lista.json')

scraper = SUAPScraper()
try:
    if scraper.setup_driver():
        resultados = scraper.processar_lista(processos)
        scraper.salvar_csv(resultados)
finally:
    scraper.fechar()
```

### M√©todo 3: Executar exemplos prontos

```bash
python exemplo_uso.py
# Escolha a op√ß√£o 4 ou 5 para usar o lista.json
```

### M√©todo 4: Teste r√°pido (apenas 3 processos)

```python
from web_scraper_suap import SUAPScraper, carregar_processos_json

processos = carregar_processos_json('lista.json')[:3]  # Apenas 3 para teste
scraper = SUAPScraper(headless=False)  # Ver o navegador funcionando
try:
    if scraper.setup_driver():
        resultados = scraper.processar_lista(processos)
        scraper.salvar_csv(resultados, 'teste.csv')
finally:
    scraper.fechar()
```

## Estrutura do Projeto

```
‚îú‚îÄ‚îÄ web_scraper_suap.py    # Script principal
‚îú‚îÄ‚îÄ exemplo_uso.py         # Exemplos de uso
‚îú‚îÄ‚îÄ lista.json            # Arquivo com n√∫meros de processo (ENTRADA)
‚îú‚îÄ‚îÄ requirements.txt       # Depend√™ncias
‚îú‚îÄ‚îÄ README.md             # Este arquivo
‚îî‚îÄ‚îÄ processos_links.csv   # Resultado (gerado ap√≥s execu√ß√£o)
```

## Formato do lista.json

O arquivo `lista.json` deve ter a seguinte estrutura:

```json
{
  "processos": [
    "23430.000701.2021-17",
    "23430.001200.2021-58",
    "23430.000986.2021-96"
  ]
}
```

## Arquivo de Sa√≠da

O script gera um CSV com as colunas:

- **NumeroProcesso**: N√∫mero do processo buscado
- **LinkProcesso**: Link direto ou "N√£o encontrado"

Exemplo:

```csv
NumeroProcesso,LinkProcesso
23430.000701.2021-17,https://suap.ifsp.edu.br/admin/processo_eletronico/processo/12345/
23430.001200.2021-58,N√£o encontrado
```

## Execu√ß√£o Passo a Passo

1. **Certifique-se que o arquivo `lista.json` existe** com seus n√∫meros de processo
2. **Execute o script principal**:
   ```bash
   python web_scraper_suap.py
   ```
3. **Fa√ßa login no navegador**:
   - O navegador Chrome abrir√° automaticamente
   - Navegue at√© o SUAP e fa√ßa seu login
   - Ap√≥s estar logado, volte ao terminal
   - Pressione ENTER para continuar
4. **Acompanhe o progresso** atrav√©s dos logs no terminal
5. **Verifique o resultado** no arquivo `processos_links.csv`

## üîê Processo de Login

O script inclui uma pausa autom√°tica para login manual:

```
üîê FA√áA SEU LOGIN NO NAVEGADOR
============================================================
1. O navegador foi aberto com a p√°gina do SUAP
2. Fa√ßa seu login normalmente
3. Navegue at√© estar logado no sistema
4. Pressione ENTER aqui para continuar...
============================================================
```

## Configura√ß√µes Avan√ßadas

### Modo Headless (sem interface gr√°fica)

```python
scraper = SUAPScraper(headless=True)
```

### Personalizar tempo de espera

```python
# No c√≥digo, altere a linha:
self.wait = WebDriverWait(self.driver, 15)  # 15 segundos
```

### Alterar intervalo entre buscas

```python
# No m√©todo processar_lista, altere:
time.sleep(2)  # 2 segundos entre cada busca
```

### Processar apenas parte da lista

```python
processos = carregar_processos_json('lista.json')
processos_parciais = processos[0:50]  # Apenas os primeiros 50
```

## Solu√ß√£o de Problemas

### Arquivo lista.json n√£o encontrado

```bash
# Verifique se o arquivo existe no mesmo diret√≥rio do script
ls lista.json

# Ou crie um arquivo de exemplo:
echo '{"processos": ["23430.000701.2021-17"]}' > lista.json
```

### ChromeDriver n√£o encontrado

```bash
# Instale o webdriver-manager
pip install webdriver-manager

# Modifique o c√≥digo para usar:
from webdriver_manager.chrome import ChromeDriverManager
service = Service(ChromeDriverManager().install())
self.driver = webdriver.Chrome(service=service, options=chrome_options)
```

### Processo n√£o encontrado

- Verifique se o n√∫mero est√° correto no `lista.json`
- Confirme se voc√™ tem acesso ao sistema SUAP
- Alguns processos podem estar restritos

### Timeout errors

- Aumente o tempo de espera
- Verifique sua conex√£o com a internet
- O servidor pode estar lento

## Boas Pr√°ticas

1. **Teste com poucos processos primeiro** (use a op√ß√£o 5 do exemplo_uso.py)
2. **Use modo headless para lotes grandes**
3. **Mantenha backup do lista.json**
4. **Monitore os logs para identificar problemas**
5. **Respeite o servidor - n√£o remova o time.sleep()**

## Exemplo Completo de Uso

```python
#!/usr/bin/env python3
from web_scraper_suap import SUAPScraper, carregar_processos_json

def main():
    # Carregar processos do JSON
    processos = carregar_processos_json('lista.json')

    if not processos:
        print("Erro: N√£o foi poss√≠vel carregar lista.json")
        return

    print(f"Carregados {len(processos)} processos")

    # Para teste, processar apenas os primeiros 5
    processos_teste = processos[:5]

    scraper = SUAPScraper(headless=False)

    try:
        if scraper.setup_driver():
            print("Iniciando coleta...")
            resultados = scraper.processar_lista(processos_teste)
            scraper.salvar_csv(resultados, 'meus_processos.csv')
            print("Conclu√≠do! Verifique o arquivo meus_processos.csv")
    finally:
        scraper.fechar()

if __name__ == "__main__":
    main()
```

## Estat√≠sticas do Arquivo Atual

O arquivo `lista.json` fornecido cont√©m **458 processos** para serem processados.

## Licen√ßa

Este projeto √© de uso livre para fins educacionais e de automa√ß√£o pessoal.
